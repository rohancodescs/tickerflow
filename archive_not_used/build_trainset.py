# build_trainset.py

'''
This script reads all of our processed parquet files generated by transform_dq.py,
and for each (symbol, date), we sort by date per symbol, compute the log returns + rolling stats, create the targets including
- target_log_return which is the next-day log return
- target_direction which is the sign of that (up, down, flat)
'''
import os
import io
import logging

import boto3
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from botocore.exceptions import ClientError

log = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(levelname)s %(asctime)s %(message)s")

S3 = boto3.client("s3")

BUCKET = os.environ.get("BUCKET", "tickerflow-data-us-east-1")
PROCESSED_PREFIX = os.environ.get("PROCESSED_PREFIX", "processed").rstrip("/")
FEATURES_PREFIX = os.environ.get("FEATURES_PREFIX", "features").rstrip("/")


def list_processed_keys(bucket: str, prefix: str):
    """
    Yield all processed parquet keys under processed/dt=*/ohlcv.parquet
    """
    paginator = S3.get_paginator("list_objects_v2")
    full_prefix = f"{prefix}/"
    for page in paginator.paginate(Bucket=bucket, Prefix=full_prefix):
        for obj in page.get("Contents", []):
            key = obj["Key"]
            if key.endswith(".parquet"):
                yield key


def load_processed_df(bucket: str, prefix: str) -> pd.DataFrame:
    """
    Load all processed parquet files from S3 into a single DataFrame.
    """
    dfs = []
    for key in list_processed_keys(bucket, prefix):
        try:
            resp = S3.get_object(Bucket=bucket, Key=key)
            body = resp["Body"].read()
            table = pq.read_table(io.BytesIO(body))
            df = table.to_pandas()
            dfs.append(df)
            log.info(f"Loaded {len(df)} rows from {key}")
        except ClientError as e:
            log.error(f"Error reading {key}: {e}")

    if not dfs:
        raise RuntimeError("No processed parquet files found.")

    df_all = pd.concat(dfs, ignore_index=True)
    # Ensure date is datetime
    df_all["date"] = pd.to_datetime(df_all["date"])
    return df_all


def make_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Build feature & target columns from processed OHLCV data.
    One row per (symbol, date) with only past data used for features.
    """
    # Sort by symbol, date
    df = df.sort_values(["symbol", "date"]).reset_index(drop=True)

    feature_frames = []

    symbols = df["symbol"].unique()
    log.info(f"Building features for symbols: {list(symbols)}")

    for sym in symbols:
        g = df[df["symbol"] == sym].copy()
        g = g.sort_values("date").reset_index(drop=True)

        # Use adjusted close for returns
        g["adj_close"] = g["adj_close"].astype(float)
        g["log_close"] = np.log(g["adj_close"])

        # 1-day log return
        g["log_ret_1d"] = g["log_close"].diff()

        # Rolling stats over returns
        windows = [5, 10, 20]
        for w in windows:
            g[f"ret_mean_{w}"] = g["log_ret_1d"].rolling(w).mean()
            g[f"ret_std_{w}"] = g["log_ret_1d"].rolling(w).std()

        # Rolling stats over close price (optional, but nice)
        for w in [5, 10]:
            g[f"close_mean_{w}"] = g["adj_close"].rolling(w).mean()
            g[f"close_std_{w}"] = g["adj_close"].rolling(w).std()

        # Calendar features
        g["day_of_week"] = g["date"].dt.weekday  # 0=Mon
        g["month"] = g["date"].dt.month

        # Targets: next-day log return and direction
        g["target_log_return"] = g["log_ret_1d"].shift(-1)
        g["target_direction"] = np.sign(g["target_log_return"]).astype("float")

        # Drop rows without full history or target
        g = g.dropna(
            subset=[
                "log_ret_1d",
                "ret_mean_5",
                "ret_std_5",
                "ret_mean_10",
                "ret_std_10",
                "target_log_return",
            ]
        )

        feature_frames.append(g)

    if not feature_frames:
        raise RuntimeError("No feature frames built (check your processed data).")

    feats = pd.concat(feature_frames, ignore_index=True)

    # Keep only needed columns
    keep_cols = [
        "symbol",
        "date",
        "adj_close",
        "volume",
        "log_ret_1d",
        "ret_mean_5",
        "ret_std_5",
        "ret_mean_10",
        "ret_std_10",
        "ret_mean_20",
        "ret_std_20",
        "close_mean_5",
        "close_std_5",
        "close_mean_10",
        "close_std_10",
        "day_of_week",
        "month",
        "target_log_return",
        "target_direction",
    ]

    # Some rolling windows may not exist if history is short; ensure they are present
    for c in keep_cols:
        if c not in feats.columns:
            feats[c] = np.nan

    feats = feats[keep_cols]

    return feats


def write_features_to_s3(df: pd.DataFrame, bucket: str, prefix: str, key_name: str = "train.parquet"):
    """
    Write the full feature dataframe to a single parquet file in S3: features/train.parquet
    """
    out_key = f"{prefix}/{key_name}"
    table = pa.Table.from_pandas(df, preserve_index=False)
    buf = io.BytesIO()
    pq.write_table(table, buf)
    buf.seek(0)

    S3.put_object(
        Bucket=bucket,
        Key=out_key,
        Body=buf.getvalue(),
        ContentType="application/octet-stream",
    )
    log.info(f"Wrote {len(df)} rows of features to s3://{bucket}/{out_key}")


def main():
    log.info(f"Loading processed data from s3://{BUCKET}/{PROCESSED_PREFIX}/")
    df_processed = load_processed_df(BUCKET, PROCESSED_PREFIX)
    log.info(f"Loaded {len(df_processed)} processed rows")

    log.info("Building feature & target dataset")
    df_features = make_features(df_processed)
    log.info(f"Built {len(df_features)} feature rows")

    write_features_to_s3(df_features, BUCKET, FEATURES_PREFIX)


if __name__ == "__main__":
    main()
