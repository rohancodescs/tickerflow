{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37417cc",
   "metadata": {},
   "source": [
    "Tickerflow - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e3de5",
   "metadata": {},
   "source": [
    "Config + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2c4c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: pyarrow in /opt/homebrew/lib/python3.10/site-packages (22.0.0)\n",
      "Requirement already satisfied: xgboost in /opt/homebrew/lib/python3.10/site-packages (3.1.2)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.10/site-packages (1.7.2)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy pyarrow xgboost joblib requests scikit-learn sklearn\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from joblib import dump\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s %(asctime)s %(message)s\")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "#CONFIG\n",
    "#----------\n",
    "\n",
    "# Tickers to backfill\n",
    "TICKERS = [\"AAPL\", \"AMZN\", \"GOOGL\", \"META\", \"MSFT\"]\n",
    "\n",
    "# November window \n",
    "START_DATE = \"2025-11-01\"\n",
    "END_DATE = \"2025-11-30\"\n",
    "\n",
    "# training window end (for metrics evaluation)\n",
    "TRAIN_END_DATE = \"2025-11-20\"\n",
    "\n",
    "#local data directory\n",
    "DATA_DIR = \"local_baseline_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "RAW_PARQUET_PATH = os.path.join(DATA_DIR, \"raw_prices_nov.parquet\")\n",
    "FEATURES_PARQUET_PATH = os.path.join(DATA_DIR, \"features_nov.parquet\")\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"models\", \"xgboost\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "FORECASTS_PARQUET_PATH = os.path.join(DATA_DIR, \"forecasts_nov.parquet\")\n",
    "\n",
    "# Alpha Vantage\n",
    "ALPHA_BASE = \"https://www.alphavantage.co/query\"\n",
    "ALPHAVANTAGE_KEY = os.environ.get(\"ALPHAVANTAGE_KEY\", \"4DQAHSBPMEBEXIXL\")\n",
    "\n",
    "# Throttle between API calls to avoid rate limit\n",
    "SLEEP_BETWEEN_CALLS_SEC = 12\n",
    "\n",
    "# Metrics tracking\n",
    "timings = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e0512",
   "metadata": {},
   "source": [
    "Ingestion helpers + backfilling data from all of November   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-01 03:48:55,077 [INGEST] Fetching AAPL...\n",
      "INFO 2025-12-01 03:48:55,250 [INGEST] AAPL -> 19 rows in November window\n",
      "INFO 2025-12-01 03:49:07,255 [INGEST] Fetching AMZN...\n",
      "INFO 2025-12-01 03:49:07,501 [INGEST] AMZN -> 19 rows in November window\n",
      "INFO 2025-12-01 03:49:19,503 [INGEST] Fetching GOOGL...\n",
      "INFO 2025-12-01 03:49:19,658 [INGEST] GOOGL -> 19 rows in November window\n",
      "INFO 2025-12-01 03:49:31,663 [INGEST] Fetching META...\n",
      "INFO 2025-12-01 03:49:31,865 [INGEST] META -> 19 rows in November window\n",
      "INFO 2025-12-01 03:49:43,870 [INGEST] Fetching MSFT...\n",
      "INFO 2025-12-01 03:49:44,039 [INGEST] MSFT -> 19 rows in November window\n",
      "INFO 2025-12-01 03:49:44,047 [INGEST] Done. Total rows=95, time=48.97s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>dividend</th>\n",
       "      <th>split_coef</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>270.420</td>\n",
       "      <td>270.850</td>\n",
       "      <td>266.250</td>\n",
       "      <td>269.05</td>\n",
       "      <td>268.790617</td>\n",
       "      <td>50194583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>alpha_vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-04</td>\n",
       "      <td>268.325</td>\n",
       "      <td>271.486</td>\n",
       "      <td>267.615</td>\n",
       "      <td>270.04</td>\n",
       "      <td>269.779663</td>\n",
       "      <td>49274846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>alpha_vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>268.610</td>\n",
       "      <td>271.700</td>\n",
       "      <td>266.930</td>\n",
       "      <td>270.14</td>\n",
       "      <td>269.879566</td>\n",
       "      <td>42586288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>alpha_vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-06</td>\n",
       "      <td>267.890</td>\n",
       "      <td>273.400</td>\n",
       "      <td>267.890</td>\n",
       "      <td>269.77</td>\n",
       "      <td>269.509923</td>\n",
       "      <td>51204045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>alpha_vantage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>269.795</td>\n",
       "      <td>272.290</td>\n",
       "      <td>266.770</td>\n",
       "      <td>268.47</td>\n",
       "      <td>268.211176</td>\n",
       "      <td>48227365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>alpha_vantage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol       date     open     high      low   close   adj_close    volume  \\\n",
       "0   AAPL 2025-11-03  270.420  270.850  266.250  269.05  268.790617  50194583   \n",
       "1   AAPL 2025-11-04  268.325  271.486  267.615  270.04  269.779663  49274846   \n",
       "2   AAPL 2025-11-05  268.610  271.700  266.930  270.14  269.879566  42586288   \n",
       "3   AAPL 2025-11-06  267.890  273.400  267.890  269.77  269.509923  51204045   \n",
       "4   AAPL 2025-11-07  269.795  272.290  266.770  268.47  268.211176  48227365   \n",
       "\n",
       "   dividend  split_coef         source  \n",
       "0       0.0         1.0  alpha_vantage  \n",
       "1       0.0         1.0  alpha_vantage  \n",
       "2       0.0         1.0  alpha_vantage  \n",
       "3       0.0         1.0  alpha_vantage  \n",
       "4       0.0         1.0  alpha_vantage  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def alpha_daily_adjusted(symbol: str, api_key: str, outputsize: str = \"compact\") -> dict:\n",
    "    \"\"\"\n",
    "    Calls/gets TIME_SERIES_DAILY_ADJUSTED table from alpha vantage and return parsed JSON.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": symbol,\n",
    "        \"outputsize\": outputsize,\n",
    "        \"datatype\": \"json\",\n",
    "        \"apikey\": api_key,\n",
    "    }\n",
    "    url = f\"{ALPHA_BASE}?{urlencode(params)}\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            with urlopen(url, timeout=20) as r:\n",
    "                body = r.read().decode(\"utf-8\")\n",
    "            js = json.loads(body)\n",
    "            if \"Note\" in js or \"Information\" in js:\n",
    "                raise RuntimeError(\n",
    "                    f\"Alpha Vantage info for {symbol}: {js.get('Note') or js.get('Information')}\"\n",
    "                )\n",
    "            if \"Error Message\" in js:\n",
    "                raise RuntimeError(f\"Alpha Vantage error for {symbol}: {js['Error Message']}\")\n",
    "            if \"Time Series (Daily)\" not in js:\n",
    "                raise RuntimeError(f\"Unexpected response for {symbol}: keys={list(js.keys())[:5]}\")\n",
    "            return js\n",
    "        except (HTTPError, URLError) as e:\n",
    "            wait = 5 * (attempt + 1)\n",
    "            log.warning(f\"HTTP error for {symbol}: {e}. Retry in {wait}s\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed to fetch after retries for {symbol}\")\n",
    "\n",
    "\n",
    "# Extracts all daily rows for [start_date, end_date] inclusive\n",
    "def extract_rows_between(symbol: str, av_json: dict,\n",
    "                         start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \n",
    "    ts = av_json.get(\"Time Series (Daily)\", {})\n",
    "    if not ts:\n",
    "        raise RuntimeError(f\"No daily time series for {symbol}\")\n",
    "\n",
    "    start_ts = pd.to_datetime(start_date)\n",
    "    end_ts = pd.to_datetime(end_date)\n",
    "\n",
    "    rows = []\n",
    "    for date_str, rec in ts.items():\n",
    "        dt = pd.to_datetime(date_str)\n",
    "        if not (start_ts <= dt <= end_ts):\n",
    "            continue\n",
    "        rows.append(\n",
    "            {\n",
    "                \"symbol\": symbol,\n",
    "                \"date\": dt,\n",
    "                \"open\": float(rec[\"1. open\"]),\n",
    "                \"high\": float(rec[\"2. high\"]),\n",
    "                \"low\": float(rec[\"3. low\"]),\n",
    "                \"close\": float(rec[\"4. close\"]),\n",
    "                \"adj_close\": float(rec.get(\"5. adjusted close\", rec[\"4. close\"])),\n",
    "                \"volume\": int(rec[\"6. volume\"]),\n",
    "                \"dividend\": float(rec.get(\"7. dividend amount\", 0.0)),\n",
    "                \"split_coef\": float(rec.get(\"8. split coefficient\", 1.0)),\n",
    "                \"source\": \"alpha_vantage\",\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Ingest November locally\n",
    "# if ALPHAVANTAGE_KEY != \"4DQAHSBPMEBEXIXL\":\n",
    "#     raise RuntimeError(\"Set ALPHAVANTAGE_KEY env var or edit ALPHAVANTAGE_KEY in the config cell.\")\n",
    "\n",
    "all_rows = []\n",
    "per_symbol_timing = {}\n",
    "\n",
    "t_ingest_start = time.perf_counter()\n",
    "\n",
    "for i, sym in enumerate(TICKERS, 1):\n",
    "    t0 = time.perf_counter()\n",
    "    log.info(f\"[INGEST] Fetching {sym}\")\n",
    "    js = alpha_daily_adjusted(sym, ALPHAVANTAGE_KEY, outputsize=\"compact\")\n",
    "    df_sym = extract_rows_between(sym, js, START_DATE, END_DATE)\n",
    "    log.info(f\"[INGEST] {sym} -> {len(df_sym)} rows in November window\")\n",
    "    all_rows.append(df_sym)\n",
    "    per_symbol_timing[sym] = time.perf_counter() - t0\n",
    "\n",
    "    if i < len(TICKERS) and SLEEP_BETWEEN_CALLS_SEC > 0:\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS_SEC)\n",
    "\n",
    "df_raw = pd.concat(all_rows, ignore_index=True)\n",
    "df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"])\n",
    "df_raw.to_parquet(RAW_PARQUET_PATH, index=False)\n",
    "\n",
    "t_ingest = time.perf_counter() - t_ingest_start\n",
    "timings[\"ingest_total_seconds\"] = t_ingest\n",
    "timings[\"ingest_per_symbol_seconds\"] = per_symbol_timing\n",
    "\n",
    "log.info(f\"[INGEST] Done. Total rows={len(df_raw)}, time={t_ingest:.2f}s\")\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ba4b8",
   "metadata": {},
   "source": [
    "Ensuring data quality and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72daece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-01 03:49:44,078 [FEATURES] Built features with 95 rows in 0.01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQ report:\n",
      "{\n",
      "  \"errors\": [],\n",
      "  \"warnings\": [],\n",
      "  \"row_count\": 95\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>dividend</th>\n",
       "      <th>split_coef</th>\n",
       "      <th>...</th>\n",
       "      <th>ret_mean_10</th>\n",
       "      <th>ret_std_10</th>\n",
       "      <th>ret_mean_20</th>\n",
       "      <th>ret_std_20</th>\n",
       "      <th>close_mean_5</th>\n",
       "      <th>close_std_5</th>\n",
       "      <th>close_mean_10</th>\n",
       "      <th>close_std_10</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>270.420</td>\n",
       "      <td>270.850</td>\n",
       "      <td>266.250</td>\n",
       "      <td>269.05</td>\n",
       "      <td>268.790617</td>\n",
       "      <td>50194583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-04</td>\n",
       "      <td>268.325</td>\n",
       "      <td>271.486</td>\n",
       "      <td>267.615</td>\n",
       "      <td>270.04</td>\n",
       "      <td>269.779663</td>\n",
       "      <td>49274846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.285140</td>\n",
       "      <td>0.699361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>268.610</td>\n",
       "      <td>271.700</td>\n",
       "      <td>266.930</td>\n",
       "      <td>270.14</td>\n",
       "      <td>269.879566</td>\n",
       "      <td>42586288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.483282</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-06</td>\n",
       "      <td>267.890</td>\n",
       "      <td>273.400</td>\n",
       "      <td>267.890</td>\n",
       "      <td>269.77</td>\n",
       "      <td>269.509923</td>\n",
       "      <td>51204045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.489942</td>\n",
       "      <td>0.491664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>269.795</td>\n",
       "      <td>272.290</td>\n",
       "      <td>266.770</td>\n",
       "      <td>268.47</td>\n",
       "      <td>268.211176</td>\n",
       "      <td>48227365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.234189</td>\n",
       "      <td>0.712986</td>\n",
       "      <td>269.234189</td>\n",
       "      <td>0.712986</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol       date     open     high      low   close   adj_close    volume  \\\n",
       "0   AAPL 2025-11-03  270.420  270.850  266.250  269.05  268.790617  50194583   \n",
       "1   AAPL 2025-11-04  268.325  271.486  267.615  270.04  269.779663  49274846   \n",
       "2   AAPL 2025-11-05  268.610  271.700  266.930  270.14  269.879566  42586288   \n",
       "3   AAPL 2025-11-06  267.890  273.400  267.890  269.77  269.509923  51204045   \n",
       "4   AAPL 2025-11-07  269.795  272.290  266.770  268.47  268.211176  48227365   \n",
       "\n",
       "   dividend  split_coef  ... ret_mean_10  ret_std_10  ret_mean_20  ret_std_20  \\\n",
       "0       0.0         1.0  ...         NaN         NaN          NaN         NaN   \n",
       "1       0.0         1.0  ...         NaN         NaN          NaN         NaN   \n",
       "2       0.0         1.0  ...         NaN         NaN          NaN         NaN   \n",
       "3       0.0         1.0  ...         NaN         NaN          NaN         NaN   \n",
       "4       0.0         1.0  ...         NaN         NaN          NaN         NaN   \n",
       "\n",
       "   close_mean_5  close_std_5  close_mean_10  close_std_10  day_of_week  month  \n",
       "0           NaN          NaN            NaN           NaN            0     11  \n",
       "1    269.285140     0.699361            NaN           NaN            1     11  \n",
       "2    269.483282     0.601942            NaN           NaN            2     11  \n",
       "3    269.489942     0.491664            NaN           NaN            3     11  \n",
       "4    269.234189     0.712986     269.234189      0.712986            4     11  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_data_quality_checks(df: pd.DataFrame):\n",
    "    report = {\"errors\": [], \"warnings\": [], \"row_count\": int(len(df))}\n",
    "    required_cols = [\"symbol\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "    if df.empty:\n",
    "        report[\"errors\"].append(\"DataFrame is empty (no raw rows found).\")\n",
    "        return False, report\n",
    "\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        report[\"errors\"].append(f\"Missing required columns: {missing}\")\n",
    "        return False, report\n",
    "\n",
    "    if df[\"date\"].isna().any():\n",
    "        n = int(df[\"date\"].isna().sum())\n",
    "        report[\"errors\"].append(f\"'date' column has {n} unparsable values.\")\n",
    "\n",
    "    for col in required_cols:\n",
    "        null_count = int(df[col].isna().sum())\n",
    "        if null_count > 0:\n",
    "            report[\"errors\"].append(f\"Column '{col}' has {null_count} null values.\")\n",
    "\n",
    "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        bad = int((df[col] <= 0).sum())\n",
    "        if bad > 0:\n",
    "            report[\"errors\"].append(f\"Column '{col}' has {bad} non-positive values.\")\n",
    "\n",
    "    bad_high_low = int((df[\"high\"] < df[\"low\"]).sum())\n",
    "    if bad_high_low > 0:\n",
    "        report[\"errors\"].append(f\"{bad_high_low} rows where high < low.\")\n",
    "\n",
    "    bad_volume = int((df[\"volume\"] < 0).sum())\n",
    "    if bad_volume > 0:\n",
    "        report[\"errors\"].append(f\"{bad_volume} rows with negative volume.\")\n",
    "\n",
    "    dupes = int(df.duplicated(subset=[\"symbol\", \"date\"]).sum())\n",
    "    if dupes > 0:\n",
    "        report[\"warnings\"].append(f\"{dupes} duplicate (symbol, date) rows.\")\n",
    "\n",
    "    ok = len(report[\"errors\"]) == 0\n",
    "    return ok, report\n",
    "\n",
    "\n",
    "def build_features(df_proc: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_proc.sort_values([\"symbol\", \"date\"]).copy()\n",
    "\n",
    "    # 1-day log return\n",
    "    df[\"log_ret_1d\"] = df.groupby(\"symbol\")[\"adj_close\"].transform(\n",
    "        lambda s: np.log(s).diff()\n",
    "    )\n",
    "\n",
    "    # Rolling stats on returns\n",
    "    for win in (5, 10, 20):\n",
    "        df[f\"ret_mean_{win}\"] = df.groupby(\"symbol\")[\"log_ret_1d\"].transform(\n",
    "            lambda s, w=win: s.rolling(w, min_periods=max(2, w // 2)).mean()\n",
    "        )\n",
    "        df[f\"ret_std_{win}\"] = df.groupby(\"symbol\")[\"log_ret_1d\"].transform(\n",
    "            lambda s, w=win: s.rolling(w, min_periods=max(2, w // 2)).std()\n",
    "        )\n",
    "\n",
    "    # Rolling stats on prices\n",
    "    for win in (5, 10):\n",
    "        df[f\"close_mean_{win}\"] = df.groupby(\"symbol\")[\"adj_close\"].transform(\n",
    "            lambda s, w=win: s.rolling(w, min_periods=max(2, w // 2)).mean()\n",
    "        )\n",
    "        df[f\"close_std_{win}\"] = df.groupby(\"symbol\")[\"adj_close\"].transform(\n",
    "            lambda s, w=win: s.rolling(w, min_periods=max(2, w // 2)).std()\n",
    "        )\n",
    "\n",
    "    # Calendar features\n",
    "    df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------ run DQ + features ------------------\n",
    "\n",
    "t_dq_start = time.perf_counter()\n",
    "ok, dq_report = run_data_quality_checks(df_raw)\n",
    "t_dq = time.perf_counter() - t_dq_start\n",
    "timings[\"dq_seconds\"] = t_dq\n",
    "\n",
    "print(\"DQ report:\")\n",
    "print(json.dumps(dq_report, indent=2, default=str))\n",
    "if not ok:\n",
    "    raise RuntimeError(\"Data quality failed; aborting pipeline.\")\n",
    "\n",
    "t_feat_start = time.perf_counter()\n",
    "df_feat = build_features(df_raw)\n",
    "t_feat = time.perf_counter() - t_feat_start\n",
    "timings[\"feature_build_seconds\"] = t_feat\n",
    "\n",
    "df_feat.to_parquet(FEATURES_PARQUET_PATH, index=False)\n",
    "log.info(f\"[FEATURES] Built features with {len(df_feat)} rows in {t_feat:.2f}s\")\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bad40a",
   "metadata": {},
   "source": [
    "Training (local run of XGBoost) + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7142cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-01 03:49:44,096 [TRAIN] 'target_log_return' not found; computing as next-day log return of adj_close.\n",
      "INFO 2025-12-01 03:49:44,099 [TRAIN] Using 13 numeric features + 5 symbol dummies\n",
      "INFO 2025-12-01 03:49:44,100 [TRAIN] Split info: {'n_dates_total': 18, 'n_train_dates': 12, 'n_val_dates': 3, 'n_test_dates': 3, 'n_train_rows': 60, 'n_val_rows': 15, 'n_test_rows': 15}\n",
      "INFO 2025-12-01 03:49:44,151 [TRAIN] Saved model to local_baseline_data/models/xgboost/LOCAL_20251201T084944Z/model.joblib\n",
      "INFO 2025-12-01 03:49:44,152 [TRAIN] Saved metrics to local_baseline_data/models/xgboost/LOCAL_20251201T084944Z/metrics.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (zero) test metrics: {'rmse': 0.014935962150597379, 'mae': 0.011520400641255304, 'smape': 1.9999950870957781, 'directional_accuracy': 0.0, 'n_samples': 15}\n",
      "XGBoost test metrics: {'rmse': 0.01877312396233493, 'mae': 0.015302628650309819, 'smape': 1.763940513081606, 'directional_accuracy': 0.2, 'n_samples': 15}\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values([\"symbol\", \"date\"])\n",
    "\n",
    "    if \"target_log_return\" not in df.columns:\n",
    "        log.info(\n",
    "            \"[TRAIN] 'target_log_return' not found; computing as next-day log return of adj_close.\"\n",
    "        )\n",
    "        if \"adj_close\" not in df.columns:\n",
    "            raise RuntimeError(\"adj_close is required to compute target_log_return.\")\n",
    "        df[\"target_log_return\"] = df.groupby(\"symbol\")[\"adj_close\"].transform(\n",
    "            lambda s: np.log(s.shift(-1) / s)\n",
    "        )\n",
    "\n",
    "    df = df.dropna(subset=[\"target_log_return\"]).copy()\n",
    "\n",
    "    base_features = [\n",
    "        \"adj_close\",\n",
    "        \"volume\",\n",
    "        \"log_ret_1d\",\n",
    "        \"ret_mean_5\",\n",
    "        \"ret_std_5\",\n",
    "        \"ret_mean_10\",\n",
    "        \"ret_std_10\",\n",
    "        \"close_mean_5\",\n",
    "        \"close_std_5\",\n",
    "        \"close_mean_10\",\n",
    "        \"close_std_10\",\n",
    "        \"day_of_week\",\n",
    "        \"month\",\n",
    "    ]\n",
    "    feature_cols = [c for c in base_features if c in df.columns]\n",
    "\n",
    "    X_num = df[feature_cols].astype(float)\n",
    "    sym_dummies = pd.get_dummies(df[\"symbol\"], prefix=\"sym\")\n",
    "    X = pd.concat([X_num, sym_dummies], axis=1)\n",
    "\n",
    "    y = df[\"target_log_return\"].astype(float).values\n",
    "    meta = df[[\"date\", \"symbol\"]].copy()\n",
    "\n",
    "    log.info(\n",
    "        f\"[TRAIN] Using {len(feature_cols)} numeric features + \"\n",
    "        f\"{sym_dummies.shape[1]} symbol dummies\"\n",
    "    )\n",
    "    return X, y, meta, list(X.columns)\n",
    "\n",
    "\n",
    "def time_based_split(X: pd.DataFrame, y: np.ndarray, meta: pd.DataFrame,\n",
    "                     train_frac: float, val_frac: float):\n",
    "    dates = np.sort(meta[\"date\"].dt.normalize().unique())\n",
    "    n_dates = len(dates)\n",
    "    if n_dates < 5:\n",
    "        raise RuntimeError(f\"Not enough unique dates ({n_dates}) for train/val/test split\")\n",
    "\n",
    "    train_end = int(n_dates * train_frac)\n",
    "    val_end = int(n_dates * (train_frac + val_frac))\n",
    "    train_dates = dates[:train_end]\n",
    "    val_dates = dates[train_end:val_end]\n",
    "    test_dates = dates[val_end:]\n",
    "\n",
    "    def mask_for_dates(date_set):\n",
    "        return meta[\"date\"].dt.normalize().isin(date_set)\n",
    "\n",
    "    m_train = mask_for_dates(train_dates)\n",
    "    m_val = mask_for_dates(val_dates)\n",
    "    m_test = mask_for_dates(test_dates)\n",
    "\n",
    "    def subset(mask):\n",
    "        return X[mask].values, y[mask], meta.loc[mask].copy()\n",
    "\n",
    "    X_tr, y_tr, meta_tr = subset(m_train)\n",
    "    X_val, y_val, meta_val = subset(m_val)\n",
    "    X_te, y_te, meta_te = subset(m_test)\n",
    "\n",
    "    split_info = {\n",
    "        \"n_dates_total\": int(n_dates),\n",
    "        \"n_train_dates\": int(len(train_dates)),\n",
    "        \"n_val_dates\": int(len(val_dates)),\n",
    "        \"n_test_dates\": int(len(test_dates)),\n",
    "        \"n_train_rows\": int(len(y_tr)),\n",
    "        \"n_val_rows\": int(len(y_val)),\n",
    "        \"n_test_rows\": int(len(y_te)),\n",
    "    }\n",
    "\n",
    "    log.info(f\"[TRAIN] Split info: {split_info}\")\n",
    "    return (X_tr, y_tr, meta_tr), (X_val, y_val, meta_val), (X_te, y_te, meta_te), split_info\n",
    "\n",
    "\n",
    "def eval_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    if len(y_true) == 0:\n",
    "        return {\n",
    "            \"rmse\": None,\n",
    "            \"mae\": None,\n",
    "            \"smape\": None,\n",
    "            \"directional_accuracy\": None,\n",
    "            \"n_samples\": 0,\n",
    "        }\n",
    "\n",
    "    err = y_pred - y_true\n",
    "    rmse = float(np.sqrt(np.mean(err ** 2)))\n",
    "    mae = float(np.mean(np.abs(err)))\n",
    "    denom = np.abs(y_true) + np.abs(y_pred) + 1e-8\n",
    "    smape = float(np.mean(2.0 * np.abs(err) / denom))\n",
    "    dir_acc = float((np.sign(y_pred) == np.sign(y_true)).mean())\n",
    "\n",
    "    return {\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"smape\": smape,\n",
    "        \"directional_accuracy\": dir_acc,\n",
    "        \"n_samples\": int(len(y_true)),\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------ run training ------------------\n",
    "\n",
    "t_prep_start = time.perf_counter()\n",
    "X, y, meta, feature_names = prepare_data(df_feat)\n",
    "t_prep = time.perf_counter() - t_prep_start\n",
    "timings[\"train_prepare_seconds\"] = t_prep\n",
    "\n",
    "# 70/15/15 time-based split (similar to your cloud flow)\n",
    "(train_set, val_set, test_set, split_info) = time_based_split(\n",
    "    X, y, meta, train_frac=0.7, val_frac=0.15\n",
    ")\n",
    "X_tr, y_tr, meta_tr = train_set\n",
    "X_val, y_val, meta_val = val_set\n",
    "X_te, y_te, meta_te = test_set\n",
    "\n",
    "# Baseline (zero-return)\n",
    "baseline_metrics = {}\n",
    "baseline_val = eval_metrics(y_val, np.zeros_like(y_val)) if len(y_val) > 0 else None\n",
    "baseline_test = eval_metrics(y_te, np.zeros_like(y_te))\n",
    "baseline_metrics[\"zero\"] = {\"val\": baseline_val, \"test\": baseline_test}\n",
    "print(\"Baseline (zero) test metrics:\", baseline_test)\n",
    "\n",
    "t_train_start = time.perf_counter()\n",
    "model = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=20 if len(y_val) > 0 else None,\n",
    ")\n",
    "\n",
    "if len(y_val) > 0:\n",
    "    model.fit(\n",
    "        X_tr,\n",
    "        y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False,\n",
    "    )\n",
    "else:\n",
    "    model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "t_train = time.perf_counter() - t_train_start\n",
    "timings[\"train_fit_seconds\"] = t_train\n",
    "\n",
    "t_eval_start = time.perf_counter()\n",
    "y_tr_pred = model.predict(X_tr)\n",
    "y_val_pred = model.predict(X_val) if len(y_val) > 0 else np.array([])\n",
    "y_te_pred = model.predict(X_te)\n",
    "t_eval = time.perf_counter() - t_eval_start\n",
    "timings[\"train_eval_seconds\"] = t_eval\n",
    "\n",
    "metrics_xgb = {\n",
    "    \"train\": eval_metrics(y_tr, y_tr_pred),\n",
    "    \"val\": eval_metrics(y_val, y_val_pred) if len(y_val) > 0 else None,\n",
    "    \"test\": eval_metrics(y_te, y_te_pred),\n",
    "}\n",
    "print(\"XGBoost test metrics:\", metrics_xgb[\"test\"])\n",
    "\n",
    "# Save local model + metrics\n",
    "run_id = datetime.now(timezone.utc).strftime(\"LOCAL_%Y%m%dT%H%M%SZ\")\n",
    "model_path = os.path.join(MODEL_DIR, run_id)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_file = os.path.join(model_path, \"model.joblib\")\n",
    "\n",
    "with open(model_file, \"wb\") as f:\n",
    "    dump(model, f)\n",
    "\n",
    "metrics_file = os.path.join(model_path, \"metrics.json\")\n",
    "metrics_payload = {\n",
    "    \"run_id\": run_id,\n",
    "    \"feature_names\": feature_names,\n",
    "    \"split_info\": split_info,\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"xgboost\": metrics_xgb,\n",
    "    \"timing_seconds\": {\n",
    "        \"prepare\": t_prep,\n",
    "        \"train\": t_train,\n",
    "        \"eval\": t_eval,\n",
    "        \"total\": t_prep + t_train + t_eval,\n",
    "    },\n",
    "}\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(metrics_payload, f, indent=2, default=str)\n",
    "\n",
    "log.info(f\"[TRAIN] Saved model to {model_file}\")\n",
    "log.info(f\"[TRAIN] Saved metrics to {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd4f3b",
   "metadata": {},
   "source": [
    "Forecasting / Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf9b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-01 03:49:44,197 [FORECAST] Predicting for 5 symbols.\n",
      "INFO 2025-12-01 03:49:44,200 [FORECAST] Wrote 5 rows to local_baseline_data/forecasts_nov.parquet in 0.04s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>as_of_date</th>\n",
       "      <th>target_date</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>pred_log_return</th>\n",
       "      <th>pred_direction</th>\n",
       "      <th>pred_adj_close</th>\n",
       "      <th>model_run_id</th>\n",
       "      <th>model_s3_key</th>\n",
       "      <th>created_ts</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>278.85</td>\n",
       "      <td>-0.004573</td>\n",
       "      <td>-1</td>\n",
       "      <td>277.577798</td>\n",
       "      <td>LOCAL_20251201T084944Z</td>\n",
       "      <td>local://local_baseline_data/models/xgboost/LOC...</td>\n",
       "      <td>2025-12-01T08:49:44.198601+00:00</td>\n",
       "      <td>2025-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>233.22</td>\n",
       "      <td>-0.001225</td>\n",
       "      <td>-1</td>\n",
       "      <td>232.934404</td>\n",
       "      <td>LOCAL_20251201T084944Z</td>\n",
       "      <td>local://local_baseline_data/models/xgboost/LOC...</td>\n",
       "      <td>2025-12-01T08:49:44.198601+00:00</td>\n",
       "      <td>2025-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>320.18</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>1</td>\n",
       "      <td>320.856306</td>\n",
       "      <td>LOCAL_20251201T084944Z</td>\n",
       "      <td>local://local_baseline_data/models/xgboost/LOC...</td>\n",
       "      <td>2025-12-01T08:49:44.198601+00:00</td>\n",
       "      <td>2025-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>META</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>647.95</td>\n",
       "      <td>-0.000419</td>\n",
       "      <td>-1</td>\n",
       "      <td>647.678843</td>\n",
       "      <td>LOCAL_20251201T084944Z</td>\n",
       "      <td>local://local_baseline_data/models/xgboost/LOC...</td>\n",
       "      <td>2025-12-01T08:49:44.198601+00:00</td>\n",
       "      <td>2025-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>492.01</td>\n",
       "      <td>-0.003120</td>\n",
       "      <td>-1</td>\n",
       "      <td>490.477272</td>\n",
       "      <td>LOCAL_20251201T084944Z</td>\n",
       "      <td>local://local_baseline_data/models/xgboost/LOC...</td>\n",
       "      <td>2025-12-01T08:49:44.198601+00:00</td>\n",
       "      <td>2025-11-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  as_of_date target_date  adj_close  pred_log_return  pred_direction  \\\n",
       "0   AAPL  2025-11-28  2025-11-29     278.85        -0.004573              -1   \n",
       "1   AMZN  2025-11-28  2025-11-29     233.22        -0.001225              -1   \n",
       "2  GOOGL  2025-11-28  2025-11-29     320.18         0.002110               1   \n",
       "3   META  2025-11-28  2025-11-29     647.95        -0.000419              -1   \n",
       "4   MSFT  2025-11-28  2025-11-29     492.01        -0.003120              -1   \n",
       "\n",
       "   pred_adj_close            model_run_id  \\\n",
       "0      277.577798  LOCAL_20251201T084944Z   \n",
       "1      232.934404  LOCAL_20251201T084944Z   \n",
       "2      320.856306  LOCAL_20251201T084944Z   \n",
       "3      647.678843  LOCAL_20251201T084944Z   \n",
       "4      490.477272  LOCAL_20251201T084944Z   \n",
       "\n",
       "                                        model_s3_key  \\\n",
       "0  local://local_baseline_data/models/xgboost/LOC...   \n",
       "1  local://local_baseline_data/models/xgboost/LOC...   \n",
       "2  local://local_baseline_data/models/xgboost/LOC...   \n",
       "3  local://local_baseline_data/models/xgboost/LOC...   \n",
       "4  local://local_baseline_data/models/xgboost/LOC...   \n",
       "\n",
       "                         created_ts          dt  \n",
       "0  2025-12-01T08:49:44.198601+00:00  2025-11-29  \n",
       "1  2025-12-01T08:49:44.198601+00:00  2025-11-29  \n",
       "2  2025-12-01T08:49:44.198601+00:00  2025-11-29  \n",
       "3  2025-12-01T08:49:44.198601+00:00  2025-11-29  \n",
       "4  2025-12-01T08:49:44.198601+00:00  2025-11-29  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_FEATURES = [\n",
    "    \"adj_close\",\n",
    "    \"volume\",\n",
    "    \"log_ret_1d\",\n",
    "    \"ret_mean_5\",\n",
    "    \"ret_std_5\",\n",
    "    \"ret_mean_10\",\n",
    "    \"ret_std_10\",\n",
    "    \"close_mean_5\",\n",
    "    \"close_std_5\",\n",
    "    \"close_mean_10\",\n",
    "    \"close_std_10\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "]\n",
    "\n",
    "\n",
    "def build_latest_feature_matrix(df: pd.DataFrame, feature_names: list[str]):\n",
    "    df_sorted = df.sort_values([\"symbol\", \"date\"])\n",
    "    latest = df_sorted.groupby(\"symbol\", as_index=False).tail(1).reset_index(drop=True)\n",
    "\n",
    "    missing_base = [c for c in BASE_FEATURES if c not in latest.columns]\n",
    "    if missing_base:\n",
    "        raise RuntimeError(f\"Missing expected feature columns: {missing_base}\")\n",
    "\n",
    "    X_num = latest[BASE_FEATURES].astype(float)\n",
    "    sym_dummies = pd.get_dummies(latest[\"symbol\"], prefix=\"sym\")\n",
    "    X = pd.concat([X_num, sym_dummies], axis=1)\n",
    "\n",
    "    # Align to training feature order\n",
    "    X = X.reindex(columns=feature_names, fill_value=0.0)\n",
    "    return latest, X.values\n",
    "\n",
    "\n",
    "# ------------------ run forecast ------------------\n",
    "\n",
    "t_forecast_start = time.perf_counter()\n",
    "\n",
    "# Reload features from disk just to mimic cloud pattern\n",
    "df_feat_loaded = pd.read_parquet(FEATURES_PARQUET_PATH)\n",
    "latest_rows, X_latest = build_latest_feature_matrix(df_feat_loaded, feature_names)\n",
    "\n",
    "log.info(f\"[FORECAST] Predicting for {len(latest_rows)} symbols.\")\n",
    "y_pred = model.predict(X_latest)\n",
    "pred_direction = np.sign(y_pred).astype(int)\n",
    "\n",
    "as_of_dates = latest_rows[\"date\"]\n",
    "target_dates = as_of_dates + pd.to_timedelta(1, unit=\"D\")\n",
    "pred_adj_close = latest_rows[\"adj_close\"].astype(float) * np.exp(y_pred)\n",
    "\n",
    "created_ts = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "forecast_df = pd.DataFrame(\n",
    "    {\n",
    "        \"symbol\": latest_rows[\"symbol\"],\n",
    "        \"as_of_date\": as_of_dates.dt.strftime(\"%Y-%m-%d\"),\n",
    "        \"target_date\": target_dates.dt.strftime(\"%Y-%m-%d\"),\n",
    "        \"adj_close\": latest_rows[\"adj_close\"].astype(float),\n",
    "        \"pred_log_return\": y_pred,\n",
    "        \"pred_direction\": pred_direction,\n",
    "        \"pred_adj_close\": pred_adj_close,\n",
    "        \"model_run_id\": run_id,\n",
    "        \"model_s3_key\": f\"local://{model_file}\",\n",
    "        \"created_ts\": created_ts,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optional dt partition column, like S3\n",
    "forecast_df[\"dt\"] = forecast_df[\"target_date\"]\n",
    "\n",
    "# Write local Parquet\n",
    "table = pa.Table.from_pandas(forecast_df)\n",
    "buf = io.BytesIO()\n",
    "pq.write_table(table, buf)\n",
    "buf.seek(0)\n",
    "with open(FORECASTS_PARQUET_PATH, \"wb\") as f:\n",
    "    f.write(buf.getvalue())\n",
    "\n",
    "t_forecast = time.perf_counter() - t_forecast_start\n",
    "timings[\"forecast_seconds\"] = t_forecast\n",
    "\n",
    "log.info(f\"[FORECAST] Wrote {len(forecast_df)} rows to {FORECASTS_PARQUET_PATH} in {t_forecast:.2f}s\")\n",
    "forecast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca37ca",
   "metadata": {},
   "source": [
    "Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15bde5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ingest_total_seconds': 48.9701365410001,\n",
      " 'ingest_per_symbol_seconds': {'AAPL': 0.17375866702059284,\n",
      "                               'AMZN': 0.2464177919900976,\n",
      "                               'GOOGL': 0.1555922089901287,\n",
      "                               'META': 0.20249587498256005,\n",
      "                               'MSFT': 0.16923533400404267},\n",
      " 'dq_seconds': 0.0009083330223802477,\n",
      " 'feature_build_seconds': 0.01022691698744893,\n",
      " 'train_prepare_seconds': 0.004221292008878663,\n",
      " 'train_fit_seconds': 0.04815529199549928,\n",
      " 'train_eval_seconds': 0.0011144999880343676,\n",
      " 'forecast_seconds': 0.04235899998457171,\n",
      " 'xgboost_test_metrics': {'rmse': 0.01877312396233493,\n",
      "                          'mae': 0.015302628650309819,\n",
      "                          'smape': 1.763940513081606,\n",
      "                          'directional_accuracy': 0.2,\n",
      "                          'n_samples': 15},\n",
      " 'baseline_zero_test_metrics': {'rmse': 0.014935962150597379,\n",
      "                                'mae': 0.011520400641255304,\n",
      "                                'smape': 1.9999950870957781,\n",
      "                                'directional_accuracy': 0.0,\n",
      "                                'n_samples': 15},\n",
      " 'split_info': {'n_dates_total': 18,\n",
      "                'n_train_dates': 12,\n",
      "                'n_val_dates': 3,\n",
      "                'n_test_dates': 3,\n",
      "                'n_train_rows': 60,\n",
      "                'n_val_rows': 15,\n",
      "                'n_test_rows': 15},\n",
      " 'model_run_id': 'LOCAL_20251201T084944Z',\n",
      " 'raw_rows': 95,\n",
      " 'feature_rows': 95,\n",
      " 'forecast_rows': 5}\n"
     ]
    }
   ],
   "source": [
    "summary = {\n",
    "    \"ingest_total_seconds\": timings.get(\"ingest_total_seconds\"),\n",
    "    \"ingest_per_symbol_seconds\": timings.get(\"ingest_per_symbol_seconds\"),\n",
    "    \"dq_seconds\": timings.get(\"dq_seconds\"),\n",
    "    \"feature_build_seconds\": timings.get(\"feature_build_seconds\"),\n",
    "    \"train_prepare_seconds\": timings.get(\"train_prepare_seconds\"),\n",
    "    \"train_fit_seconds\": timings.get(\"train_fit_seconds\"),\n",
    "    \"train_eval_seconds\": timings.get(\"train_eval_seconds\"),\n",
    "    \"forecast_seconds\": timings.get(\"forecast_seconds\"),\n",
    "    \"xgboost_test_metrics\": metrics_xgb.get(\"test\"),\n",
    "    \"baseline_zero_test_metrics\": baseline_metrics[\"zero\"][\"test\"],\n",
    "    \"split_info\": split_info,\n",
    "    \"model_run_id\": run_id,\n",
    "    \"raw_rows\": int(len(df_raw)),\n",
    "    \"feature_rows\": int(len(df_feat)),\n",
    "    \"forecast_rows\": int(len(forecast_df)),\n",
    "}\n",
    "\n",
    "import pprint\n",
    "pprint.pp(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b22ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local max memory used ~ 192.1 MB\n",
      "Local max memory used ~ 0.2 GB\n"
     ]
    }
   ],
   "source": [
    "import resource  \n",
    "\n",
    "usage = resource.getrusage(resource.RUSAGE_SELF)\n",
    "max_rss_bytes = usage.ru_maxrss          # max resident set size\n",
    "max_rss_kb = max_rss_bytes / 1024       # rough MB\n",
    "max_rss_mb = max_rss_kb / 1024\n",
    "max_rss_gb =max_rss_mb / 1024\n",
    "print(f\"Local max memory used ~ {max_rss_mb:.1f} MB\")\n",
    "print(f\"Local max memory used ~ {max_rss_gb:.1f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e378762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
